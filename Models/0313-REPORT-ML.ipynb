{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using Machine Learning (ML) in Genetic Algorithms</center>\n",
    "\n",
    "<center>by Katrina Gensterblum</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Machine learning is a powerful computational tool that can be used to solve a variety of problems. It has the ability to solve everything from prediction to classification. One major part of nearly all machine learning algorithms are hyperparameters. Hyperparameters are the parameters in an algorithm that the user defines themself before an experiment is conducted. Common examples of hyperparameters in machine learning algorithms include learning rates, number of iterations, or thresholds. Depending on how these values are set, the performance of an algorithm can change drastically [1]. Often small changes in hyperparameters result in large performance changes of an algorithm. For example, changing a learning rate from 0.01 to 0.1 can take a converging machine algorithm and make it completely useless.  \n",
    "\n",
    "The magnitude of importance in choosing good hyperparameters is not lost on scientists today. Researchers in any field that utilizes machine learning, realize that the ability to find good hyperparameters for their problem can determine the success of their model. Even more, finding the optimal hyperparameters for their model can allow it to produce amazing results. Unfortunately, finding the optimal hyperparameters is not an easy task. Many studies have been done in an attempt to develop a quick and accurate method to perform hyperparameter tuning, each with varying degrees of success [2].  \n",
    "\n",
    "One example of a method to perform hyperparameter turning is a genetic algorithm [3]. Genetic algorithms are basically machine learning for machine learning. Through a combination of random and organized searching, genetic algorithms are often able to converge to a solution for hyperparameters that is better than random guessing. While the solution is not necessarily the optimal one, it is systematically better than trying to achieve a solution through simple trial and error.  \n",
    "\n",
    "For our research, we utilize genetic algorithms in the context of image segmentation. Not only are we using genetic algorithms as a machine learning tool to find optimal parameters for a previously defined method, we are using genetic algorithms to find the best method overall. The genetic algorithm searches through a list of image segmentation methods and returns the best method with the best hyperparameters it can find for it.  \n",
    "\n",
    "It is in this way, that our research already utilizes machine learning. As the genetic algorithm runs for more iterations, the performance of it improves. It learns what algorithms and hyperparameters are most effective at image segmentation for a given set of images. Evidence of this learning can be seen in both the resulting fitness of a population, and the individuals contained in this population. As a genetic algorithm runs, the overall error of the population can be seen to always decrease or remain the same. Intuitively, this signals that the algorithm is most likely learning what individuals in the population produce better results. However, since there is always an element of randomness with genetic algorithms, this is not a guarantee. This is where we can look at the individuals in the population and see that indeed, the algorithm is learning. When the genetic algorithm begins, the population is made up of random individuals, but in later generations, the population is made up of variations of the individual with the current best solution. Between this, and the decreasing fitness, we can see that the genetic algorithm component of our research is in fact learning.  \n",
    "\n",
    "Our research already relies on machine learning in order to achieve a solution. Genetic algorithms are the main form of machine learning we use, and we use them two-fold: first to find the best image segmentation method for a set of images, and second to find the best hyperparameters for that method. It is through this use of machine learning for machine learning, that we can achieve impressive results for our problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "[\\[1\\]](https://arxiv.org/pdf/1502.02127.pdf) Claesen, Marc; Bart De Moor. \"Hyperparameter Search in Machine Learning.\" (2015).  \n",
    "\n",
    "[\\[2\\]](https://ieeexplore.ieee.org/document/8297018) Bochinski, Erik, et al. “Hyper-Parameter Optimization for Convolutional Neural Network Committees Based on Evolutionary Algorithms.” 2017 IEEE International Conference on Image Processing (ICIP), 2017.  \n",
    "\n",
    "[\\[3\\]](https://link.springer.com/content/pdf/10.1007/3-540-44673-7_7.pdf) Shapiro, Jonathan. “Genetic Algorithms in Machine Learning.” Machine Learning and Its Applications Lecture Notes in Computer Science, 2001, pp. 146–168.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
